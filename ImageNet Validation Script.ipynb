{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MPF Evaluation Framework: ImageNet Validation Script w/ FiftyOne\n",
    "\n",
    "This script provides an example of the FiftyOne tool for processing ImageNet Results.\n",
    "Please feel free to use and modify the script as needed for investigations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "# NOTICE                                                                    #\n",
    "#                                                                           #\n",
    "# This software (or technical data) was produced for the U.S. Government    #\n",
    "# under contract, and is subject to the Rights in Data-General Clause       #\n",
    "# 52.227-14, Alt. IV (DEC 2007).                                            #\n",
    "#                                                                           #\n",
    "# Copyright 2021 The MITRE Corporation. All Rights Reserved.                #\n",
    "#############################################################################\n",
    "\n",
    "#############################################################################\n",
    "# Copyright 2021 The MITRE Corporation                                      #\n",
    "#                                                                           #\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");           #\n",
    "# you may not use this file except in compliance with the License.          #\n",
    "# You may obtain a copy of the License at                                   #\n",
    "#                                                                           #\n",
    "#    http://www.apache.org/licenses/LICENSE-2.0                             #\n",
    "#                                                                           #\n",
    "# Unless required by applicable law or agreed to in writing, software       #\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,         #\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  #\n",
    "# See the License for the specific language governing permissions and       #\n",
    "# limitations under the License.                                            #\n",
    "#############################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import fiftyone as fo\n",
    "import csv\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import datetime\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify ImageNet Dataset, MPF JSON Predictions Directory, and Related Annotation Files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to ImageNet Dataset Directory\n",
    "dataset_dir = \"/home/mpf/G_Drive/ImageNet/val\"\n",
    "\n",
    "# Path to dataset annotations file or directory (csv for ImageNet).\n",
    "annotations_file = \"/home/mpf/G_Drive/ImageNet/ClipMetrics/data/LOC_val_solution.csv\"\n",
    "\n",
    "# Path to class labels file (classes.txt for ImageNet).\n",
    "class_mapping_file = \"/home/mpf/G_Drive/ImageNet/ClipMetrics/data/classifications.txt\"\n",
    "\n",
    "# Path to predictions directory containing job run subdirectories.\n",
    "predictions_dir = \"/home/mpf/G_Drive/ImageNet/ClipMetrics/outputs/\"\n",
    "\n",
    "# Output path for prediction metrics.\n",
    "output_metrics = \"./results.txt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specifies number of files to process in dataset.\n",
    "# Set below 0 to process the entire dataset, or between 0-1 to process a percentage of the dataset.\n",
    "dataset_subsample = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are optional parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset type (Currently supports \"imagenet\")\n",
    "dataset_type = \"imagenet\"\n",
    "\n",
    "#Enable FiftyOne Interface:\n",
    "view_fiftyone = True\n",
    "view_fiftyone_port = 5151\n",
    "\n",
    "# Enable reporting of all individual class metrics.\n",
    "# Warning, label file can be hard to read.\n",
    "verbose = False\n",
    "\n",
    "# Checks if class labels should be standardized to lower case.\"\n",
    "case_sensitive = True\n",
    "\n",
    "# Suppresses print warnings.\n",
    "suppress_warnings = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MPF Evaluation Framework Class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvalFramework:\n",
    "    def __enter__(self):\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, *exc_info):\n",
    "        return\n",
    "\n",
    "    def __init__(self,\n",
    "                 dataset_dir: str = None,\n",
    "                 annotations_file: str = None,\n",
    "                 predictions_dir: str = None,\n",
    "                 dataset_type: str = None,\n",
    "                 output_metrics: str = None,\n",
    "                 view_fiftyone: bool = True,\n",
    "                 class_mapping_file: str = None,\n",
    "                 case_sensitive: bool = True,\n",
    "                 dataset_sample: float = -1,\n",
    "                 verbose: bool = False,\n",
    "                 suppress_warnings = True,\n",
    "                 ):\n",
    "        \n",
    "        self.dataset_dir = dataset_dir\n",
    "        self.annotations_file = annotations_file\n",
    "        self.predictions_dir = predictions_dir\n",
    "        # TODO: Add support for other dataset types\n",
    "        self.dataset_type = dataset_type\n",
    "        self.class_mapping_file = class_mapping_file\n",
    "\n",
    "        self.view_fiftyone = view_fiftyone\n",
    "        self.case_sensitive = case_sensitive\n",
    "        self.output_metrics = output_metrics\n",
    "\n",
    "        self.prediction_tags = []\n",
    "\n",
    "        self.dataset_sample = dataset_sample\n",
    "\n",
    "        self.class_to_string = {}\n",
    "        self.class_id_list = []\n",
    "        self.annotations = []\n",
    "        self.dataset = fo.Dataset()\n",
    "        self.results = {}\n",
    "        self.verbose = verbose\n",
    "        self.suppress_warnings = suppress_warnings\n",
    "\n",
    "        if self.dataset_type == \"imagenet\":\n",
    "            self._init_imagenet_data()\n",
    "\n",
    "    def _init_imagenet_data(self):\n",
    "        \"\"\"\n",
    "        Initialize the ImageNet class labels list and ImageNet ground truth annotations.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        self._setup_class_lists_imagenet()\n",
    "        self._setup_imagenet_annotations()\n",
    "\n",
    "    def _setup_class_lists_imagenet(self):\n",
    "        \"\"\"\n",
    "        Initialize the ImageNet class labels list.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        with open(self.class_mapping_file, \"r+\") as f:\n",
    "            for id_string in f.read().split(\"\\n\"):\n",
    "                id_list = id_string.split(\" \")\n",
    "                self.class_to_string[id_list[0]] = id_string[len(id_list[0]):].strip()\n",
    "                if not self.case_sensitive:\n",
    "                    self.class_to_string[id_list[0]] = self.class_to_string[id_list[0]].lower()\n",
    "                self.class_id_list.append(self.class_to_string[id_list[0]])\n",
    "\n",
    "    def _setup_imagenet_annotations(self):\n",
    "        \"\"\"\n",
    "        Initialize the ImageNet groundtruth dataset annotations.\n",
    "        Based on code by Zachary Cafego.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        with open(self.annotations_file) as f:\n",
    "            csv_reader = csv.reader(f, delimiter=\",\")\n",
    "            line_count = 0\n",
    "            for row in csv_reader:\n",
    "                line_count += 1\n",
    "                if line_count > 1:\n",
    "                    filename = row[0]\n",
    "                    classification = row[1].split()[0]\n",
    "                    filepath = os.path.join(self.dataset_dir, filename+'.JPEG')\n",
    "                    if not os.path.exists(filepath):\n",
    "                        continue\n",
    "                    self.annotations.append(dict(filename=filename, filepath=filepath, classification=classification))\n",
    "\n",
    "    def setup_dataset(self):\n",
    "        \"\"\"\n",
    "        Setup ground truth dataset for image classification.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        samples = []\n",
    "        for annotation in self.annotations:\n",
    "            filepath = annotation[\"filepath\"]\n",
    "            label = annotation[\"classification\"]\n",
    "            sample = fo.Sample(filepath=filepath)\n",
    "\n",
    "            # Store classification in a field name of your choice\n",
    "            label = self.class_to_string[label]\n",
    "            if not self.case_sensitive:\n",
    "                label = label.lower()\n",
    "\n",
    "            sample[\"ground_truth\"] = fo.Classification(label=label)\n",
    "            samples.append(sample)\n",
    "\n",
    "        dataset_samples = samples\n",
    "\n",
    "        # Trim dataset if user enters a fraction or non-negative number of files.\n",
    "        if self.dataset_sample > 0:\n",
    "            if self.dataset_sample < 1:\n",
    "                dataset_sample = len(samples) * self.dataset_sample\n",
    "            dataset_samples = random.sample(samples, dataset_sample)\n",
    "        self.dataset.add_samples(dataset_samples)\n",
    "\n",
    "    def _create_logits(self, top_preds, class_list):\n",
    "        \"\"\"\n",
    "        Create a logits prediction vector given a list of top predictions.\n",
    "\n",
    "        :param top_preds:\n",
    "        :param class_list:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        logits = np.full(len(class_list), sys.float_info.min)\n",
    "        for pred in top_preds:\n",
    "            # print(pred)\n",
    "            logits[class_list.index(pred[0])] = pred[1]\n",
    "        return np.log(logits)\n",
    "\n",
    "    def add_predictions(self):\n",
    "        \"\"\"\n",
    "        Add predictions from each job to dataset.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # Construct a logits array and fill in top predictions by model.\n",
    "        # All missing predictions are reconstructed with lowest possible values.\n",
    "        predictions_list = [f.path for f in os.scandir(self.predictions_dir) if f.is_dir()]\n",
    "\n",
    "        for pred in predictions_list:\n",
    "\n",
    "            pred_tag = pred.split(\"/\")[-1]\n",
    "            self.prediction_tags.append(pred_tag)\n",
    "            for sample in self.dataset:\n",
    "                filepath = sample[\"filepath\"]\n",
    "                filename = filepath.split(\"/\")[-1][:-5]\n",
    "                jfile = os.path.join(pred, filename + \".json\")\n",
    "                if os.path.exists(jfile):\n",
    "                    if str(filepath) in self.dataset:\n",
    "                        with open(jfile) as f:\n",
    "                            output = json.load(f)\n",
    "                            top_classes = output[\"media\"][0][\"output\"][\"CLASS\"][0][\"tracks\"][0][\"detections\"][0][\n",
    "                                \"detectionProperties\"][\"CLASSIFICATION LIST\"]\n",
    "                            top_conf = output[\"media\"][0][\"output\"][\"CLASS\"][0][\"tracks\"][0][\"detections\"][0][\n",
    "                                \"detectionProperties\"][\"CLASSIFICATION CONFIDENCE LIST\"]\n",
    "                            top_conf = [float(f.strip(\" tensor()\")) for f in top_conf.split(\";\")]\n",
    "                            if not self.case_sensitive:\n",
    "                                top_classes = [x.lower() for x in top_classes.split(\"; \")]\n",
    "                            else:\n",
    "                                top_classes = top_classes.split(\"; \")\n",
    "\n",
    "                            logits = self._create_logits(zip(top_classes, top_conf), self.class_id_list)\n",
    "\n",
    "                            sample = self.dataset[str(filepath)]\n",
    "                            sample[pred_tag] = fo.Classification(\n",
    "                                label=top_classes[0],\n",
    "                                confidence=top_conf[0],\n",
    "                                logits=logits,\n",
    "                            )\n",
    "                            sample.save()\n",
    "                    else:\n",
    "                        if not self.suppress_warnings:\n",
    "                            print(\"Warning: Missing Groundtruth for \" + filepath)\n",
    "                else:\n",
    "                    if not self.suppress_warnings:\n",
    "                        print(\"Warning: Missing Prediction for \" + filepath)\n",
    "                        self.dataset.remove_sample(sample)\n",
    "\n",
    "    def evaluate_fiftyone(self):\n",
    "        \"\"\"\n",
    "        Evaluate top k predictions for each job run.\n",
    "        K values range from 1-10 and can be adjusted for further inferences.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        #TODO: Continue updating evaluation framework metrics.\n",
    "        k_pred = [1, 3, 5, 10]\n",
    "        for pred in self.prediction_tags:\n",
    "            results_dict = {}\n",
    "            for k in k_pred:\n",
    "                results = self.dataset.evaluate_classifications(\n",
    "                    pred,\n",
    "                    gt_field=\"ground_truth\",\n",
    "                    eval_key=\"eval_\" + pred.replace(\"-\",\"_\") + \"_k_{}\".format(k),\n",
    "                    method=\"top-k\",\n",
    "                    classes=self.class_id_list,\n",
    "                    k=k,\n",
    "                )\n",
    "                results_dict[k] = results.report(classes=self.class_id_list)\n",
    "\n",
    "            if self.output_metrics is not None:\n",
    "                self.results[pred] = results_dict\n",
    "\n",
    "    def launch_fiftyone_session(self, port: int = 5151):\n",
    "        \"\"\"\n",
    "        Launch a FiftyOne session if prompted by the user.\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        session = fo.launch_app(self.dataset, port=port)\n",
    "        closing = input(\"Would you like to close this app (y/n):\")\n",
    "        while closing.lower()[0] != \"y\":\n",
    "            closing = input(\"Would you like to close this app (y/n):\")\n",
    "\n",
    "    def generate_summary(self):\n",
    "        \"\"\"\n",
    "        Print out summary of metric results.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        Path(self.output_metrics).parent.mkdir(parents=True, exist_ok=True)\n",
    "        if len(self.results) > 0:\n",
    "            with open(self.output_metrics, \"w+\") as f:\n",
    "                f.write(\"MPF Evaluation Summary:\\n\")\n",
    "                f.write(\"Date: {}\\n\".format(datetime.datetime.now().strftime(\"%Y_%m_%d-%I_%M_%S_%p\")))\n",
    "\n",
    "                for job_name, result in self.results.items():\n",
    "                    f.write(\"\\n\\nRun Name: {}\\n\".format(job_name))\n",
    "                    for k in result:\n",
    "                        f.write(\"Prediction Metrics (top-k = {}): \\n\\n\".format(k))\n",
    "                        if self.verbose:\n",
    "                            metrics = pd.DataFrame(result[k]).transpose()\n",
    "                        else:\n",
    "                            aggregate_res = [\"micro avg\", \"macro avg\", \"weighted avg\"]\n",
    "                            metrics = {}\n",
    "                            for agg in aggregate_res:\n",
    "                                metrics[agg] = result[k][agg]\n",
    "                            metrics = pd.DataFrame(metrics).transpose()\n",
    "                        f.write(metrics.to_csv(sep=\"\\t\"))\n",
    "                        f.write(\"\\n\")\n",
    "                        \n",
    "        return self.results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize evalutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = EvalFramework(dataset_dir=dataset_dir,\n",
    "                       annotations_file=annotations_file,\n",
    "                       predictions_dir=predictions_dir,\n",
    "                       dataset_type=dataset_type,\n",
    "                       class_mapping_file=class_mapping_file,\n",
    "                       output_metrics=output_metrics,\n",
    "                       view_fiftyone=view_fiftyone,\n",
    "                       case_sensitive=case_sensitive,\n",
    "                       dataset_sample=dataset_subsample,\n",
    "                       verbose=verbose\n",
    "                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup dataset and add predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |███████████████| 5023/5023 [5.1s elapsed, 0s remaining, 969.5 samples/s]       \n"
     ]
    }
   ],
   "source": [
    "evaluator.setup_dataset()\n",
    "evaluator.add_predictions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator.evaluate_fiftyone()\n",
    "if output_metrics is not None:\n",
    "    metrics  = evaluator.generate_summary()\n",
    "if view_fiftyone:\n",
    "    evaluator.launch_fiftyone_session(port=view_fiftyone_port)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
